# -*- coding: utf-8 -*-
"""
masters/llm_master.py ‚Äî LLM-–º–∞—Å—Ç–µ—Ä VTuber —Å–∏—Å—Ç–µ–º—ã

–£–ø—Ä–∞–≤–ª—è–µ—Ç:
- –ì–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –æ—Ç–≤–µ—Ç–æ–≤ (HybridOllamaRouter)
- Ollama –∫–ª–∏–µ–Ω—Ç–æ–º –∏ –∫—ç—à–µ–º
- –î–µ—Ç–µ–∫—Ü–∏–µ–π —ç–º–æ—Ü–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞
- –†–æ—É—Ç–∏–Ω–≥–æ–º –º–µ–∂–¥—É fast/smart –º–æ–¥–µ–ª—è–º–∏

–í–µ—Ä—Å–∏—è: 1.0 (2025-11-03)
"""

from __future__ import annotations

import asyncio
import logging
from typing import Optional, Tuple, Dict

from .base import BaseMaster

# –ò–º–ø–æ—Ä—Ç—ã LLM-–º–æ–¥—É–ª–µ–π
from llm.router import HybridOllamaRouter
from llm.ollama_client import OptimizedOllamaClient

# –ò–º–ø–æ—Ä—Ç—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
from core.config import VTuberConfig

logger = logging.getLogger("MasterLLM")


class MasterLLM(BaseMaster):
    """
    LLM-–º–∞—Å—Ç–µ—Ä ‚Äî —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –æ—Ç–≤–µ—Ç–æ–≤ –∏ —Ä–æ—É—Ç–∏–Ω–≥–æ–º –º–æ–¥–µ–ª–µ–π.
    
    –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
    - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ —á–µ—Ä–µ–∑ Ollama
    - –†–æ—É—Ç–∏–Ω–≥ –º–µ–∂–¥—É fast/smart –º–æ–¥–µ–ª—è–º–∏
    - –î–µ—Ç–µ–∫—Ü–∏—è —ç–º–æ—Ü–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞
    - –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤
    
    API:
    - generate_reply() ‚Äî –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å —ç–º–æ—Ü–∏–µ–π
    - generate_streaming() ‚Äî –ø–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
    - detect_emotion() ‚Äî –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —ç–º–æ—Ü–∏—é –∏–∑ —Ç–µ–∫—Å—Ç–∞
    """
    
    def __init__(
        self,
        config: Optional[VTuberConfig] = None,
        fast_model: Optional[str] = None,
        smart_model: Optional[str] = None,
    ):
        super().__init__("LLM")
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        self.config = config or VTuberConfig.load()
        
        # –ú–æ–¥–µ–ª–∏ (–º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å)
        self._fast_model = fast_model or self.config.fast_model
        self._smart_model = smart_model or self.config.smart_model
        
        # –ü–æ–¥—Å–∏—Å—Ç–µ–º—ã (–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–∏ start)
        self.ollama_client: Optional[OptimizedOllamaClient] = None
        self.router: Optional[HybridOllamaRouter] = None
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self._generation_count = 0
        self._fast_model_usage = 0
        self._smart_model_usage = 0
        self._error_count = 0
    
    async def _start_internal(self) -> None:
        """–ó–∞–ø—É—Å–∫ Ollama –∫–ª–∏–µ–Ω—Ç–∞ –∏ —Ä–æ—É—Ç–µ—Ä–∞"""
        try:
            # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Ollama –∫–ª–∏–µ–Ω—Ç–∞
            self.ollama_client = OptimizedOllamaClient()
            self.logger.info("‚úÖ Ollama –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
            # 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ä–æ—É—Ç–µ—Ä–∞
            self.router = HybridOllamaRouter(
                ollama=self.ollama_client,
                fast_model=self._fast_model,
                smart_model=self._smart_model,
            )
            self.logger.info(
                f"‚úÖ LLM —Ä–æ—É—Ç–µ—Ä –≥–æ—Ç–æ–≤: fast={self._fast_model}, smart={self._smart_model}"
            )
            
            self.logger.info("üß† MasterLLM –ø–æ–ª–Ω–æ—Å—Ç—å—é –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ LLM: {e}", exc_info=True)
            raise
    
    async def _stop_internal(self) -> None:
        """–ó–∞–∫—Ä—ã—Ç–∏–µ Ollama –∫–ª–∏–µ–Ω—Ç–∞"""
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.logger.info(
            f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ LLM: generations={self._generation_count}, "
            f"fast={self._fast_model_usage}, smart={self._smart_model_usage}, "
            f"errors={self._error_count}"
        )
        
        # –ó–∞–∫—Ä—ã–≤–∞–µ–º –∫–ª–∏–µ–Ω—Ç
        if self.ollama_client:
            try:
                await self.ollama_client.close()
                self.logger.info("‚úÖ Ollama –∫–ª–∏–µ–Ω—Ç –∑–∞–∫—Ä—ã—Ç")
            except Exception as e:
                self.logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–∫—Ä—ã—Ç–∏—è Ollama: {e}")
    
    async def health_check(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è LLM-–ø–æ–¥—Å–∏—Å—Ç–µ–º"""
        if not self._running:
            return False
        
        checks = {
            "ollama_client": self.ollama_client is not None,
            "router": self.router is not None,
        }
        
        all_ok = all(checks.values())
        if not all_ok:
            self.logger.warning(f"‚ö†Ô∏è Health check failed: {checks}")
        
        return all_ok
    
    # ==================== API: –ì–ï–ù–ï–†–ê–¶–ò–Ø –û–¢–í–ï–¢–û–í ====================
    
    async def generate_reply(
        self,
        user_text: str,
        context: Optional[Dict] = None,
        system_prompt: Optional[str] = None,
    ) -> Tuple[str, str]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –¥–µ—Ç–µ–∫—Ü–∏–µ–π —ç–º–æ—Ü–∏–∏.
        
        Args:
            user_text: —Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            context: –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞ (turns + facts)
            system_prompt: —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        
        Returns:
            (reply_text, emotion_name)
        
        Raises:
            RuntimeError: –µ—Å–ª–∏ MasterLLM –Ω–µ –∑–∞–ø—É—â–µ–Ω
        """
        if not self.router:
            raise RuntimeError("MasterLLM –Ω–µ –∑–∞–ø—É—â–µ–Ω (—Ä–æ—É—Ç–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)")
        
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
            context_str = ""
            if context:
                turns = context.get("turns", [])
                context_str = "\n".join([
                    f"{t['role']}: {t['text']}" for t in turns[-10:]
                ])
                full_prompt = f"{context_str}\n\nuser: {user_text}"
            else:
                full_prompt = user_text
            
            # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            if not system_prompt:
                system_prompt = "–¢—ã ‚Äî –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π VTuber-–∫–æ–º–ø–∞–Ω—å–æ–Ω. –û–±—â–∞–π—Å—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ."
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            reply, emotion = await self.router.generate_reply(
                full_prompt,
                context=context_str,
                system_prompt=system_prompt
            )
            
            self._generation_count += 1
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫–∞—è –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å (–ø—Ä–∏–º–µ—Ä–Ω–æ)
            if len(user_text.split()) > 18:
                self._smart_model_usage += 1
            else:
                self._fast_model_usage += 1
            
            self.logger.debug(
                f"ü§ñ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –æ—Ç–≤–µ—Ç: {reply[:50]}... [{emotion}]"
            )
            
            return reply, emotion
            
        except Exception as e:
            self._error_count += 1
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}", exc_info=True)
            raise
    
    async def generate_streaming(
        self,
        user_text: str,
        system_prompt: Optional[str] = None,
    ):
        """
        –ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ (–¥–ª—è TTS –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏).
        
        Args:
            user_text: —Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            system_prompt: —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        
        Yields:
            –ß–∞–Ω–∫–∏ —Ç–µ–∫—Å—Ç–∞ –ø–æ –º–µ—Ä–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        
        Raises:
            RuntimeError: –µ—Å–ª–∏ MasterLLM –Ω–µ –∑–∞–ø—É—â–µ–Ω
        """
        if not self.router:
            raise RuntimeError("MasterLLM –Ω–µ –∑–∞–ø—É—â–µ–Ω (—Ä–æ—É—Ç–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)")
        
        try:
            if not system_prompt:
                system_prompt = "–¢—ã ‚Äî –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π VTuber-–∫–æ–º–ø–∞–Ω—å–æ–Ω."
            
            async for chunk in self.router.ask_streaming(user_text, system_prompt):
                yield chunk
            
            self._generation_count += 1
            
        except Exception as e:
            self._error_count += 1
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ—Ç–æ–∫–æ–≤–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            raise
    
    # ==================== API: –î–ï–¢–ï–ö–¶–ò–Ø –≠–ú–û–¶–ò–ô ====================
    
    def detect_emotion(self, text: str) -> str:
        """
        –î–µ—Ç–µ–∫—Ü–∏—è —ç–º–æ—Ü–∏–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ (—É–ø—Ä–æ—â—ë–Ω–Ω–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞).
        
        Args:
            text: —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
        Returns:
            –ù–∞–∑–≤–∞–Ω–∏–µ —ç–º–æ—Ü–∏–∏ (happy/sad/angry/surprised/neutral)
        """
        if not self.router:
            return "neutral"
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ —Ä–æ—É—Ç–µ—Ä–∞
            emotion = self.router._detect_emotion(text, "")
            self.logger.debug(f"üòä –î–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∞ —ç–º–æ—Ü–∏—è: {emotion}")
            return emotion
        
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ —ç–º–æ—Ü–∏–∏: {e}")
            return "neutral"
    
    # ==================== API: –ü–†–Ø–ú–û–ô –î–û–°–¢–£–ü ====================
    
    async def ask_fast(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
    ) -> str:
        """
        –ü—Ä—è–º–æ–π –∑–∞–ø—Ä–æ—Å –∫ fast-–º–æ–¥–µ–ª–∏ (–º–∏–Ω—É—è —Ä–æ—É—Ç–µ—Ä).
        
        Args:
            prompt: –ø—Ä–æ–º–ø—Ç
            system_prompt: —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        
        Returns:
            –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
        """
        if not self.ollama_client:
            raise RuntimeError("MasterLLM –Ω–µ –∑–∞–ø—É—â–µ–Ω")
        
        try:
            response = await self.ollama_client.generate(
                prompt=prompt,
                system=system_prompt,
                params={"model": self._fast_model}
            )
            self._fast_model_usage += 1
            return response
        
        except Exception as e:
            self._error_count += 1
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ fast-–º–æ–¥–µ–ª–∏: {e}")
            raise
    
    async def ask_smart(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
    ) -> str:
        """
        –ü—Ä—è–º–æ–π –∑–∞–ø—Ä–æ—Å –∫ smart-–º–æ–¥–µ–ª–∏ (–º–∏–Ω—É—è —Ä–æ—É—Ç–µ—Ä).
        
        Args:
            prompt: –ø—Ä–æ–º–ø—Ç
            system_prompt: —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        
        Returns:
            –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
        """
        if not self.ollama_client:
            raise RuntimeError("MasterLLM –Ω–µ –∑–∞–ø—É—â–µ–Ω")
        
        try:
            response = await self.ollama_client.generate(
                prompt=prompt,
                system=system_prompt,
                params={"model": self._smart_model}
            )
            self._smart_model_usage += 1
            return response
        
        except Exception as e:
            self._error_count += 1
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ smart-–º–æ–¥–µ–ª–∏: {e}")
            raise
    
    # ==================== –£–¢–ò–õ–ò–¢–´ ====================
    
    def get_stats(self) -> dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–∞–±–æ—Ç—ã LLM"""
        return {
            "running": self._running,
            "fast_model": self._fast_model,
            "smart_model": self._smart_model,
            "generation_count": self._generation_count,
            "fast_usage": self._fast_model_usage,
            "smart_usage": self._smart_model_usage,
            "error_count": self._error_count,
        }
    
    def get_models(self) -> dict:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª—è—Ö"""
        return {
            "fast": self._fast_model,
            "smart": self._smart_model,
        }
